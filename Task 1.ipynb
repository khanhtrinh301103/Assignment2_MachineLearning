{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ede927-bd54-4841-be73-13816a178f45",
   "metadata": {},
   "source": [
    "# Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "732332e7-393d-40f4-b797-cb8b191b069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#model selection\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Add,Dropout, Dense, Activation, ZeroPadding2D, \\\n",
    "BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "\n",
    "\n",
    "#preprocess.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#dl libraraies\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# specifically for cnn\n",
    "from keras.layers import Dropout, Flatten,Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# specifically for manipulating zipped images and getting numpy arrays of pixel values of images.\n",
    "import cv2                  \n",
    "import numpy as np  \n",
    "from tqdm import tqdm\n",
    "import os                   \n",
    "from random import shuffle  \n",
    "from zipfile import ZipFile\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "import glob   \n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35db15c-a15b-4255-b3ce-87b8a8541a2d",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "556191de-c0c8-4ae0-9eab-b24f9d036448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beds - Asian has 52 images\n",
      "beds - Beach has 51 images\n",
      "beds - Contemporary has 1953 images\n",
      "beds - Craftsman has 188 images\n",
      "beds - Eclectic has 22 images\n",
      "beds - Farmhouse has 91 images\n",
      "beds - Industrial has 61 images\n",
      "beds - Mediterranean has 75 images\n",
      "beds - Midcentury has 130 images\n",
      "beds - Modern has 380 images\n",
      "beds - Rustic has 239 images\n",
      "beds - Scandinavian has 33 images\n",
      "beds - Southwestern has 49 images\n",
      "beds - Traditional has 1397 images\n",
      "beds - Transitional has 1719 images\n",
      "beds - Tropical has 55 images\n",
      "beds - Victorian has 83 images\n",
      "chairs - Asian has 381 images\n",
      "chairs - Beach has 186 images\n",
      "chairs - Contemporary has 4608 images\n",
      "chairs - Craftsman has 210 images\n",
      "chairs - Eclectic has 201 images\n",
      "chairs - Farmhouse has 640 images\n",
      "chairs - Industrial has 534 images\n",
      "chairs - Mediterranean has 187 images\n",
      "chairs - Midcentury has 3585 images\n",
      "chairs - Modern has 1788 images\n",
      "chairs - Rustic has 304 images\n",
      "chairs - Scandinavian has 216 images\n",
      "chairs - Southwestern has 95 images\n",
      "chairs - Traditional has 3956 images\n",
      "chairs - Transitional has 4514 images\n",
      "chairs - Tropical has 430 images\n",
      "chairs - Victorian has 218 images\n",
      "dressers - Asian has 484 images\n",
      "dressers - Beach has 166 images\n",
      "dressers - Contemporary has 1284 images\n",
      "dressers - Craftsman has 128 images\n",
      "dressers - Eclectic has 41 images\n",
      "dressers - Farmhouse has 567 images\n",
      "dressers - Industrial has 216 images\n",
      "dressers - Mediterranean has 109 images\n",
      "dressers - Midcentury has 307 images\n",
      "dressers - Modern has 256 images\n",
      "dressers - Rustic has 378 images\n",
      "dressers - Scandinavian has 11 images\n",
      "dressers - Southwestern has 35 images\n",
      "dressers - Traditional has 2086 images\n",
      "dressers - Transitional has 1622 images\n",
      "dressers - Tropical has 47 images\n",
      "dressers - Victorian has 133 images\n",
      "lamps - Asian has 1572 images\n",
      "lamps - Beach has 2820 images\n",
      "lamps - Contemporary has 5343 images\n",
      "lamps - Craftsman has 2020 images\n",
      "lamps - Eclectic has 942 images\n",
      "lamps - Farmhouse has 1154 images\n",
      "lamps - Industrial has 1837 images\n",
      "lamps - Mediterranean has 1012 images\n",
      "lamps - Midcentury has 874 images\n",
      "lamps - Modern has 2335 images\n",
      "lamps - Rustic has 832 images\n",
      "lamps - Scandinavian has 94 images\n",
      "lamps - Southwestern has 138 images\n",
      "lamps - Traditional has 4789 images\n",
      "lamps - Transitional has 5066 images\n",
      "lamps - Tropical has 271 images\n",
      "lamps - Victorian has 1302 images\n",
      "sofas - Asian has 7 images\n",
      "sofas - Beach has 17 images\n",
      "sofas - Contemporary has 1240 images\n",
      "sofas - Craftsman has 13 images\n",
      "sofas - Eclectic has 13 images\n",
      "sofas - Farmhouse has 21 images\n",
      "sofas - Industrial has 17 images\n",
      "sofas - Mediterranean has 9 images\n",
      "sofas - Midcentury has 647 images\n",
      "sofas - Modern has 152 images\n",
      "sofas - Rustic has 15 images\n",
      "sofas - Scandinavian has 15 images\n",
      "sofas - Southwestern has 20 images\n",
      "sofas - Traditional has 599 images\n",
      "sofas - Transitional has 1201 images\n",
      "sofas - Tropical has 28 images\n",
      "sofas - Victorian has 66 images\n",
      "tables - Asian has 573 images\n",
      "tables - Beach has 494 images\n",
      "tables - Contemporary has 3177 images\n",
      "tables - Craftsman has 409 images\n",
      "tables - Eclectic has 410 images\n",
      "tables - Farmhouse has 1279 images\n",
      "tables - Industrial has 2661 images\n",
      "tables - Mediterranean has 582 images\n",
      "tables - Midcentury has 1194 images\n",
      "tables - Modern has 2272 images\n",
      "tables - Rustic has 71 images\n",
      "tables - Scandinavian has 45 images\n",
      "tables - Southwestern has 27 images\n",
      "tables - Traditional has 3629 images\n",
      "tables - Transitional has 207 images\n",
      "tables - Tropical has 24 images\n",
      "tables - Victorian has 46 images\n",
      "Processed 72023 training files.\n",
      "Processed 18059 testing files.\n"
     ]
    }
   ],
   "source": [
    "# Define the base, training, and testing directories\n",
    "data_dir = os.curdir + \"/Furniture_Data\"\n",
    "training_dir = os.curdir + '/Train'\n",
    "testing_dir = os.curdir + '/Test'\n",
    "\n",
    "# Set the ratio of training to testing data\n",
    "train_test_ratio = 0.8 \n",
    "\n",
    "def split_dataset_into_test_and_train_sets(all_data_dir=data_dir, training_data_dir=training_dir,\n",
    "                                           testing_data_dir=testing_dir, train_test_ratio=0.8):\n",
    "    # Ensure the testing and training directories exist\n",
    "    if not os.path.exists(training_data_dir):\n",
    "        os.makedirs(training_data_dir)\n",
    "\n",
    "    if not os.path.exists(testing_data_dir):\n",
    "        os.makedirs(testing_data_dir)\n",
    "\n",
    "    num_training_files = 0\n",
    "    num_testing_files = 0\n",
    "\n",
    "    # Walk through the directory structure\n",
    "    for type_dir in os.listdir(all_data_dir):\n",
    "        type_path = os.path.join(all_data_dir, type_dir)\n",
    "        if os.path.isdir(type_path):  # Ensure it's a directory\n",
    "            for style_dir in os.listdir(type_path):\n",
    "                style_path = os.path.join(type_path, style_dir)\n",
    "                if os.path.isdir(style_path):  # Check if it's a directory\n",
    "                    # Create corresponding training and testing directories\n",
    "                    training_style_dir = os.path.join(training_data_dir, type_dir, style_dir)\n",
    "                    testing_style_dir = os.path.join(testing_data_dir, type_dir, style_dir)\n",
    "                    os.makedirs(training_style_dir, exist_ok=True)\n",
    "                    os.makedirs(testing_style_dir, exist_ok=True)\n",
    "\n",
    "                    # Get all JPEG files in the current style directory\n",
    "                    file_list = glob.glob(os.path.join(style_path, '*.jpg'))\n",
    "\n",
    "                    print(f\"{type_dir} - {style_dir} has {len(file_list)} images\")\n",
    "                    random_set = np.random.permutation(file_list)\n",
    "\n",
    "                    # Split the files into training and testing sets\n",
    "                    train_list = random_set[:int(len(random_set) * train_test_ratio)]\n",
    "                    test_list = random_set[int(len(random_set) * train_test_ratio):]\n",
    "\n",
    "                    # Copy files to the respective directories\n",
    "                    for file_path in train_list:\n",
    "                        shutil.copy(file_path, training_style_dir)\n",
    "                        num_training_files += 1\n",
    "\n",
    "                    for file_path in test_list:\n",
    "                        shutil.copy(file_path, testing_style_dir)\n",
    "                        num_testing_files += 1\n",
    "\n",
    "    print(f\"Processed {num_training_files} training files.\")\n",
    "    print(f\"Processed {num_testing_files} testing files.\")\n",
    "\n",
    "# Example function call\n",
    "split_dataset_into_test_and_train_sets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad497d-6eb7-49a4-a6cf-127441932149",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b7d3588-cb6b-40f6-8279-11301b1ab80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54019 images belonging to 6 classes.\n",
      "Found 18004 images belonging to 6 classes.\n",
      "Found 18059 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the common image size to the most frequent size in the dataset\n",
    "image_size = 350  # Use the most common resolution for minimal distortion\n",
    "batch_size = 64  # Maintain a batch size of 64 for optimal GPU utilization\n",
    "\n",
    "# Initialize ImageDataGenerators\n",
    "# Only rescale is needed as all images are RGB and have the same number of channels\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,  # Reduced to 0.2 for smaller shifts at a larger resolution\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,  # Slightly increased to allow for more variation\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.25  # Using 25% of the training data for validation\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.25\n",
    ")\n",
    "\n",
    "test_data_gen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# Define paths to your training and testing directories\n",
    "training_dir = './Train'\n",
    "testing_dir = './Test'\n",
    "\n",
    "# Setup data generators\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_generator = val_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    testing_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deade6b-04d6-484f-b7f7-a2701d6c1de1",
   "metadata": {},
   "source": [
    "# CNN Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cf1cc01-9f78-4269-9e24-f36696eb6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model(image_size=350, num_classes=6):\n",
    "    baseModel = Sequential()\n",
    "    \n",
    "    # Explicit Input layer\n",
    "    baseModel.add(Input(shape=(image_size, image_size, 3)))\n",
    "    \n",
    "    # First convolutional layer\n",
    "    baseModel.add(Conv2D(filters=32, kernel_size=(5,5), padding='Same', activation='relu'))\n",
    "    \n",
    "    # Pooling layer\n",
    "    baseModel.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # Additional convolutional and pooling layers\n",
    "    baseModel.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))\n",
    "    baseModel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "    baseModel.add(Conv2D(filters=96, kernel_size=(3,3), padding='Same', activation='relu'))\n",
    "    baseModel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "    baseModel.add(Conv2D(filters=96, kernel_size=(3,3), padding='Same', activation='relu'))\n",
    "    baseModel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "    # Flattening the output of the convolutional layers to feed into the dense layer\n",
    "    baseModel.add(Flatten())\n",
    "\n",
    "    # Dense layer\n",
    "    baseModel.add(Dense(512, activation='relu'))\n",
    "\n",
    "    # Output layer with units equal to the number of categories\n",
    "    baseModel.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model with updated optimizer syntax\n",
    "    baseModel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return baseModel\n",
    "\n",
    "# Example of initializing the model\n",
    "model = get_base_model(image_size=350, num_classes=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430eb205-278e-4a83-960e-c9ce3f95c780",
   "metadata": {},
   "source": [
    "## Fit non-balanced dataset to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afae0a7a-eea4-4f14-aa4c-c4c90ba4c437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "844/844 [==============================] - 3914s 5s/step - loss: 1.0160 - accuracy: 0.6167 - val_loss: 0.7637 - val_accuracy: 0.7224\n",
      "Epoch 2/20\n",
      "844/844 [==============================] - 3987s 5s/step - loss: 0.6458 - accuracy: 0.7700 - val_loss: 0.5274 - val_accuracy: 0.8164\n",
      "Epoch 3/20\n",
      " 83/844 [=>............................] - ETA: 56:29 - loss: 0.5143 - accuracy: 0.8189"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "rawBaseModel = get_base_model(num_classes=6)  # Adjust num_classes based on your categories\n",
    "\n",
    "# Training the model using the correct data generators\n",
    "rawBaseHistory = rawBaseModel.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // batch_size,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_generator.n // batch_size,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rawBaseHistory.history['accuracy'])\n",
    "plt.plot(rawBaseHistory.history['val_accuracy'])\n",
    "plt.title('Model Accuracy - Furniture Dataset')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d922ab-ddfe-457c-8ef3-4d1c2f95b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_lR_batch(learning_rate, num_classes):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Explicit Input layer\n",
    "    model.add(Input(shape=(350, 350, 3)))\n",
    "    \n",
    "    # First convolutional layer\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), padding='Same', activation='relu'))\n",
    "    \n",
    "    # Pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # Additional convolutional and pooling layers\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters=96, kernel_size=(3,3), padding='Same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters=96, kernel_size=(3,3), padding='Same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "    # Flattening the output of the convolutional layers to feed into the dense layer\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Dense layer\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "\n",
    "    # Output layer with units equal to the number of categories\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model with updated optimizer syntax\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4daf76-bd4e-4b3b-8a0b-f83a7ee46efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54019 images belonging to 6 classes.\n",
      "Found 18004 images belonging to 6 classes.\n",
      "Found 18059 images belonging to 6 classes.\n",
      "Epoch 1/20\n",
      "   5/1688 [..............................] - ETA: 56:16 - loss: 1.7116 - accuracy: 0.2875"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "lr_boundary = [1e-5, 1e-4,1e-3,1e-2]\n",
    "\n",
    "escape_for_loop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=3, restore_best_weights=True)\n",
    "\n",
    "train32_generator = train_data_gen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid32_generator = val_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test32_generator = test_data_gen.flow_from_directory(\n",
    "    testing_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "for learning_rate in lr_boundary:\n",
    "    model = CNN_lR_batch(learning_rate, 6)\n",
    "    tracking = model.fit(\n",
    "    train32_generator,\n",
    "    steps_per_epoch=train32_generator.n // 32,\n",
    "    validation_data=valid32_generator,\n",
    "    validation_steps=valid32_generator.n // 32,\n",
    "    epochs=20,\n",
    "    verbose=1, callbacks=[escape_for_loop]\n",
    "    )\n",
    "\n",
    "    plt.plot(tracking.history['accuracy'])\n",
    "    plt.plot(tracking.history['val_accuracy'])\n",
    "    plt.title('Accuracy with ' + str(learning_rate) + ' learning rate and ' + str(batch_size) +' batch size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "    models.append(model)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fdf278-8cce-4443-84a0-392638503c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "lr_boundary = [1e-5, 1e-4,1e-3,1e-2]\n",
    "\n",
    "escape_for_loop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=3, restore_best_weights=True)\n",
    "\n",
    "train64_generator = train_data_gen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid64_generator = val_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test64_generator = test_data_gen.flow_from_directory(\n",
    "    testing_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "for learning_rate in lr_boundary:\n",
    "    model = CNN_lR_batch(learning_rate, 6)\n",
    "    tracking = model.fit(\n",
    "    train_generator64,\n",
    "    steps_per_epoch=train64_generator.n // 64,\n",
    "    validation_data=valid64_generator,\n",
    "    validation_steps=valid_64generator.n // 64,\n",
    "    epochs=20,\n",
    "    verbose=1, callbacks=[escape_for_loop]\n",
    "    )\n",
    "\n",
    "    plt.plot(tracking.history['accuracy'])\n",
    "    plt.plot(tracking.history['val_accuracy'])\n",
    "    plt.title('Accuracy with ' + str(learning_rate) + ' learning rate and ' + str(batch_size) +' batch size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "    models.append(model)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcaa204-a971-45f1-9b5c-674b4938af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "lr_boundary = [1e-5, 1e-4,1e-3,1e-2]\n",
    "\n",
    "escape_for_loop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=3, restore_best_weights=True)\n",
    "\n",
    "train128_generator = train_data_gen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=128,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid128_generator = val_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=128,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test128_generator = test_data_gen.flow_from_directory(\n",
    "    testing_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=128,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "for learning_rate in lr_boundary:\n",
    "    model = CNN_lR_batch(learning_rate, 6)\n",
    "    tracking = model.fit(\n",
    "    train128_generator,\n",
    "    steps_per_epoch=train128_generator.n // 128,\n",
    "    validation_data=valid128_generator,\n",
    "    validation_steps=valid128_generator.n // 128,\n",
    "    epochs=20,\n",
    "    verbose=1, callbacks=[escape_for_loop]\n",
    "    )\n",
    "\n",
    "    plt.plot(tracking.history['accuracy'])\n",
    "    plt.plot(tracking.history['val_accuracy'])\n",
    "    plt.title('Accuracy with ' + str(learning_rate) + ' learning rate and ' + str(batch_size) +' batch size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "    models.append(model)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73081701-8cc7-4f08-a053-ca1ff258bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "lr_boundary = [1e-5, 1e-4,1e-3,1e-2]\n",
    "\n",
    "escape_for_loop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=3, restore_best_weights=True)\n",
    "\n",
    "train256_generator = train_data_gen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=256,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid256_generator = val_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=256,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test256_generator = test_data_gen.flow_from_directory(\n",
    "    testing_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=256,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "for learning_rate in lr_boundary:\n",
    "    model = CNN_lR_batch(learning_rate, 6)\n",
    "    tracking = model.fit(\n",
    "    train256_generator,\n",
    "    steps_per_epoch=train256_generator.n // 256,\n",
    "    validation_data=valid256_generator,\n",
    "    validation_steps=valid256_generator.n // 256,\n",
    "    epochs=20,\n",
    "    verbose=1, callbacks=[escape_for_loop]\n",
    "    )\n",
    "\n",
    "    plt.plot(tracking.history['accuracy'])\n",
    "    plt.plot(tracking.history['val_accuracy'])\n",
    "    plt.title('Accuracy with ' + str(learning_rate) + ' learning rate and ' + str(batch_size) +' batch size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "    models.append(model)\n",
    "            \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
